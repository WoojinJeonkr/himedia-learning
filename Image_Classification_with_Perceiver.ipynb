{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_Classification_with_Perceiver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSrFR1egM3+kNw9r4dmus7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WoojinJeonkr/DeepLearning/blob/main/Image_Classification_with_Perceiver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceiver를 이용한 이미지 분류\n",
        "- 내용 출처: https://keras.io/examples/vision/perceiver_image_classification/\n",
        "- 사용 데이터 세트: CIFAR-100 데이터 세트\n",
        "- 목표: Andrew Jaegle et al. 의 Perceiver: Iterative Attention을 사용한 일반 인식 모델 구현"
      ],
      "metadata": {
        "id": "4y7ZM-2XqCVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceiver란?\n",
        "- 내용 출처: https://yhkim4504.tistory.com/14\n",
        "- 구글 딥마인드에서 제안한 기존 Perceiver의 단점을 보완한 모델\n",
        "- 비대칭 주의 메커니즘을 활용하여 입력을 반복적으로 좁은 잠재 병목 상태로 증류하여 매우 큰 입력을 처리하도록 확장 가능\n",
        "- 어떠한 데이터 형태도 처리할 수 있으면서 계산과 메모리 사용이 입력 사이즈에 선형적으로 작동하지만(기존의 transformer는 데이터가 클수록 느려짐) 간단한 output 형태만 출력 가능\n",
        "- NLP, Vision, Audio, Multi-modal"
      ],
      "metadata": {
        "id": "OlhDWkgxslvw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEnHQiXep-K2",
        "outputId": "fb1c1c8b-32c9-4b69-f898-f1826908f7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01. 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "bTexZYAX1Net"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "SyfTAKGauMeN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02. 데이터 로드"
      ],
      "metadata": {
        "id": "uTUpRs9C1sRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 100\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOHw0vIl1S5a",
        "outputId": "0a426da7-40de-40f5-d0cf-33dda40c22b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03. 하이퍼파라미터 구성"
      ],
      "metadata": {
        "id": "50quPaU212IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "dropout_rate = 0.2\n",
        "image_size = 64\n",
        "patch_size = 2\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "latent_dim = 256\n",
        "projection_dim = 256\n",
        "num_heads = 8\n",
        "ffn_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2\n",
        "classifier_units = [\n",
        "    projection_dim,\n",
        "    num_classes,\n",
        "]"
      ],
      "metadata": {
        "id": "mdNBzcEJ1x1F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CIq-xE22FOW",
        "outputId": "c91eb3f6-f561-4863-b8c3-9edfc2c8f829"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 64 X 64 = 4096\n",
            "Patch size: 2 X 2 = 4 \n",
            "Patches per image: 1024\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 1024 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04. 데이터 보강"
      ],
      "metadata": {
        "id": "FT2OgH2-2Ix4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "metadata": {
        "id": "YopQFMR22F-t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05. 피드포워드 네트워크(FFN) 구성\n",
        "- 내용 출처: [딥러닝 피드 포워드 (feed forward)](https://m.blog.naver.com/beyondlegend/221373971859)\n",
        "- 피드포워드(Feed Forward): 입력 층으로 데이터가 입력되고, 1개 이상으로 구성되는 은닉층을 거쳐서 마지막에 있는 출력층으로 출력값을 내보내는 과정\n",
        "=> 이전 층에서 나온 출력값이 층과 층 사이에 적용되는 가중치 영향을 받은 다음 다음층의 입력 값으로 들어 가는 것\n"
      ],
      "metadata": {
        "id": "w-Ob4R6y2RtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ffn(hidden_units, dropout_rate):\n",
        "    ffn_layers = []\n",
        "    for units in hidden_units[:-1]:\n",
        "        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n",
        "    ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "    ffn = keras.Sequential(ffn_layers)\n",
        "    return ffn"
      ],
      "metadata": {
        "id": "y_ZGtwiM2ObF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06. 레이어로 패치 생성 구현"
      ],
      "metadata": {
        "id": "hsF5erLt3cRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "nUoyPjkV3a9j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07. 패치 인코딩 계층 구현"
      ],
      "metadata": {
        "id": "i63oxMOi3kJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patches):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "lG1Rzjvf3iVp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08. Perceiver 모델 구현\n",
        "- 교차주의 모듈과 자가주의가 있는 표준 변환기의 두 가지 모듈로 구성"
      ],
      "metadata": {
        "id": "zW_3En2q34V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 교차주의 모듈\n",
        "1. (latent_dim, projection_dim)잠재 배열을 예상하고 (data_dim, projection_dim)데이터 배열을 입력으로 예상하여 잠재 배열 (latent_dim, projection_dim)을 출력으로 생성\n",
        "2. 교차 주의를 적용하기 위해 query벡터는 잠재 배열에서 생성 key되고 및 value벡터는 인코딩된 이미지에서 생성"
      ],
      "metadata": {
        "id": "45QocaHK4k6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n",
        "):\n",
        "\n",
        "    inputs = {\n",
        "        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n",
        "        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n",
        "    }\n",
        "\n",
        "    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n",
        "    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "    query = layers.Dense(units=projection_dim)(latent_array)\n",
        "    key = layers.Dense(units=projection_dim)(data_array)\n",
        "    value = layers.Dense(units=projection_dim)(data_array)\n",
        "\n",
        "    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n",
        "        [query, key, value], return_attention_scores=False\n",
        "    )\n",
        "\n",
        "    attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n",
        "    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "    outputs = ffn(attention_output)\n",
        "    outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "GjQQE_vD3san"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 변압기 모듈\n",
        "- 교차 주의 모듈의 출력 잠재 벡터를 입력으로 예상하고 다중 헤드 자체 주의를 해당 latent_dim요소에 적용한 다음 피드포워드 네트워크를 적용하여 다른 (latent_dim, projection_dim)잠재 배열을 생성"
      ],
      "metadata": {
        "id": "CdpTZ4RD4nyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transformer_module(\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "):\n",
        "\n",
        "    inputs = layers.Input(shape=(latent_dim, projection_dim))\n",
        "    x0 = inputs\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, x0])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n",
        "        x3 = ffn(x3)\n",
        "        x0 = layers.Add()([x3, x2])\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=x0)\n",
        "    return model"
      ],
      "metadata": {
        "id": "fCdgRuzg4gGy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 감지기 모델\n",
        "- 잠재적인 배열이 필요에 따라 반복적으로 입력 이미지에서 정보를 추출할 수 있도록 공유 가중치 및 건너뛰기 연결을 사용 하여 교차 주의 및 변환기 모듈 시간을 반복"
      ],
      "metadata": {
        "id": "ns4SfEyp45GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # latent array 생성\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # 패치 모델 생성\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # 패치 인코더 생성\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "\n",
        "       # 교차주의 어텐션 모델 생성\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # 트랜스포머 모델 생성\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        augmented = data_augmentation(inputs)\n",
        "        patches = self.patcher(augmented)\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        for _ in range(self.num_iterations):\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "g8p5P_w243xx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09. 모드 컴파일, 훈련 및 평가"
      ],
      "metadata": {
        "id": "KjDJEAww5bg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.LAMB(\n",
        "        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n",
        "    )\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.2, patience=3\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "dxrQGcTK5Z35"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "metadata": {
        "id": "OWPj6i235zrJ"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}